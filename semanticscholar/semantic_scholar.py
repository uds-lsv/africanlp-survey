# -*- coding: utf-8 -*-
"""Copy of semantic_scholar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/
"""

import argparse
import pandas as pd
import os
import time
from datetime import datetime
import requests

def get_batch2(query = "africa", year="2018-2024", venue = "arXiv.org"):
  # Define the query parameters
  query_params = {"fields": "title,year,abstract,url,year"}
  if venue == "AfricaNLP":
    url = f"https://api.semanticscholar.org/graph/v1/paper/search/bulk?year={year}&venue={venue}"
  else:
    url = f"https://api.semanticscholar.org/graph/v1/paper/search/bulk?query={query}&year={year}&venue={venue}"
  # Directly define the API key (Reminder: Securely handle API keys in production environments)
  api_key = "your api key goes here"  # Replace with the actual API key

  # Define headers with API key
  headers = {"x-api-key": api_key}

  # Send the API request
  response = requests.get(url, params=query_params)#, headers=headers)
  response_data = {}
  # Check response status
  if response.status_code == 200:
    response_data = response.json()
    # Process and print the response data as needed
    #print(response_data)
  else:
    print(f"Request failed with status code {response.status_code}: {response.text}")
  if 'data' in response_data:
    return response_data['data']
  else:
    return {}

def get_batch(query="africa", year="2019-2024", venue="arXiv.org"):
    # Define the query parameters in the payload
    query_params = {
        "fields": "title,year,abstract,url",
    }
    
    if venue == "AfricaNLP":
        url = f"https://api.semanticscholar.org/graph/v1/paper/search/bulk?year={year}&venue={venue}"
    else:
        url = f"https://api.semanticscholar.org/graph/v1/paper/search/bulk?query={query}&year={year}&venue={venue}"

    
    # Determine the URL based on the venue
    # url = "https://api.semanticscholar.org/graph/v1/paper/search/bulk"
    
    # Define the API key (ensure this is stored securely in production)
    api_key = "your api key goes here"  # Replace with the actual API key

    # Define headers with API key
    headers = {"x-api-key": api_key}
    headers = {
            'Authorization' : "tNKw"
    }

    # Attempt up to 3 tries
    max_retries = 3
    for attempt in range(max_retries):
        # Send the POST request
        response = requests.get(url, params=query_params, headers=headers)
        
        # Check response status
        if response.status_code == 200:
            response_data = response.json()
            return response_data['data']
        else:
            print(f"Attempt {attempt + 1} failed with status code {response.status_code}: {response.text}")
            # Wait before retrying
            time.sleep(2)  # 2-second delay before next retry
    
    # If all attempts fail, return an empty dictionary
    print(f"All attempts failed for {query} at venue {venue}.")
    return {}

def write_merge(file_paths, filename):
  #file_paths = ["NLP_Conference.xlsx", "AI_Conference.xlsx", "Speech_Conference.xlsx"]
  # Create a new Excel writer object
  with pd.ExcelWriter(filename) as writer:
      # Loop over each file and save it to a new sheet
      for file_path in file_paths:
          print(file_path)
          # Read each Excel file
          df = pd.read_excel(file_path)

          # Get the file name (without extension) to use as the sheet name
          sheet_name = file_path.split('.')[0]  # Extract the file name without '.xlsx'

          # Write to a new sheet in the writer object
          df.to_excel(writer, sheet_name=sheet_name, index=False)

  print("All files have been merged into 'merged_file.xlsx'")

def read_csv(filename="your_file.csv"):
  df = pd.read_csv(filename)
  return df

def main(args):
  # Get the current date and time, format it as a string
  current_time = datetime.now().strftime("%Y-%m") #-%d_%H") #-%M-%S")
  # Define the folder name
  folder_name = f"{current_time}"

  # Create the folder
  os.makedirs(folder_name, exist_ok=True)

  datadiv = {
      "AI_Conference": ["AAAI", "IJCAI"],
      "ARXIV":["arXiv.org"],
      "HCI_Conference": ["CHI", "AfriCHI", "CSCW Companion", "ACM Symposium on User Interface Software and Technology"],
      "IR_Knowledge": ["SIGIR", "CIKM", "Web Search and Data Mining", "The Web Conference", "Text Retrieval Conference"],
      "NLP_Conference": ['ACL', 'NAACL', "EACL", "EMNLP", "COLING", "LREC", "RANLP", "AACL","IJCNLP"],
      "ML_Conference": ['Neurips', 'ICML', "ICLR", "FAccT"],
      "NLP_Journals": ["ACM Transactions on Asian and Low-Resource Language Information Processing", "TACL", "Language Resources and Evaluation"],
      "Speech_Conference": ["ICASSP", "INTERSPEECH", "SLT", "Automatic Speech Recognition & Understanding", "IWSLT"],
      "Workshops": ["WMT", "AfricaNLP", "MRL", "Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing", "WOAH", "International Workshop on Semantic Evaluation", "Conference on Computational Natural Language Learning"],
      "Speech_Journal": ["Computer Speech and Language", "IEEE/ACM Transactions on Audio Speech and Language Processing", "Speech Communication"],
      "ACL_Summit": ["Machine Translation Summit", "EAMT"],
      }

  category = args.category
  # Create the folder
  os.makedirs(f"{folder_name}/{category}", exist_ok=True)

  if category == "africa":
    queryterms = ["africa"]
  elif category == "country":
    df = read_csv('Countries-Continents.csv')
    # Filter for rows where Continent is "Africa" and get unique countries
    african_countries = df[df["Continent"] == "Africa"]["Country"].unique()
    queryterms = set([item.strip().lower() for item in african_countries])
  elif category == "language":
    terms = read_csv('african_languages.csv')['Languages']
    queryterms = set([item.strip().lower() for item in terms])

  for source in datadiv:
    #if source != "Speech_Conference" and source != "Speech_Journal" and source != "IR_Knowledge":
    #continue
    result = []
    for venue in datadiv[source]:
      #if venue == "AfricaNLP" and category != "africa":
      #  continue
      for query in queryterms:
        if venue == "Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing":
          ven = "DeepLo"
        elif venue == "ACM Transactions on Asian and Low-Resource Language Information Processing":
          ven = "TALLIP"
        elif venue == "CSCW Companion":
          ven = "CSCW"
        elif venue == "ACM Symposium on User Interface Software and Technology":
          ven = "UIST"
        elif venue == "IEEE/ACM Transactions on Audio Speech and Language Processing":
          ven = "IEEE TASLP"
        elif venue == "Automatic Speech Recognition & Understanding":
          ven = "ASRU"
        elif venue == "International Workshop on Semantic Evaluation":
            ven = "SemEval"
        elif venue == "Conference on Computational Natural Language Learning":
            ven =  "CoNLL"
        else:
          ven = venue
        retdata = get_batch(query = query, year="2019-2024", venue = venue)
        #qt = [query]*len(retdata)
        result.extend([[res['paperId'], res['title'], res['abstract'], res['url'], res['year'], ven, query] for res in retdata])
        time.sleep(5) #30)
    # write result
    df = pd.DataFrame(result, columns=["paperId", "title", "abstract", "url", "year", "venue", "query"])
    # Sort by columns "B" then "C"
    df = df.sort_values(by=["year", "venue"], ascending=False)
    df["year"] = df["year"].astype(str)
    df.to_excel(f"{folder_name}/{category}/{source}.xlsx", index=False)
    time.sleep(30) 

if __name__ == "__main__":
  # Create the parser
  parser = argparse.ArgumentParser(description="Process a single optional parameter.")
  # Add one optional argument with "--" prefix
  parser.add_argument("--category", help="Input parameter to process")
  # Parse the argument
  args = parser.parse_args()
  main(args)
